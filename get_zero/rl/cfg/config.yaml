
# Task name - used to pick the class to load
task_name: ${task.name}
# experiment name. defaults to name of training config
experiment: ''

# if set to positive integer, overrides the default number of environments
num_envs: ''

# seed - set to -1 to choose random seed
seed: 42
# set to True for deterministic performance
torch_deterministic: False

# set the maximum number of learning iterations to train for. overrides default per-environment setting
max_iterations: ''

## Device config
#  'physx' or 'flex'
physics_engine: 'physx'
# whether to use cpu or gpu pipeline
pipeline: 'gpu'
# if set, then overrides sim_device, rl_device, and graphics_device_id to all be the specified GPU
gpu: -1
# device for running physics simulation
sim_device: 'cuda:0'
# device to run RL
rl_device: 'cuda:0'
graphics_device_id: 0 # see train.py for notes on how this is indexed to map from vulkan to cuda ordering
smart_device_selection: True # automatically sets CUDA_VISIBLE_DEVICES and reorders graphics device to match CUDA ordering; see train.py for details

## PhysX arguments
num_threads: 4 # Number of worker threads per scene used by PhysX - for CPU PhysX only.
solver_type: 1 # 0: pgs, 1: tgs
num_subscenes: 4 # Splits the simulation into N physics scenes and runs each one in a separate thread

# RLGames Arguments
# if set, run policy in inference mode without training (requires setting checkpoint to load). If not set, then performs both train and test
test: False
# how many times to step environment while testing trained policy
test_steps: 500
# used to set checkpoint path
checkpoint: ''
# set sigma when restoring network
sigma: ''
# set to True to use multi-gpu training
multi_gpu: False

wandb_activate: True
wandb_group: ''
wandb_name: ${train.params.config.name}
wandb_entity: null
wandb_project: 'get_zero'
wandb_tags: []
wandb_logcode_dir: ''
wandb_log_freq: 1

pbt: # population based training is not used in this project
  enabled: False

# during testing, the entire run will be captured if capture_video is True
headless: True
record_video: True
record_video_source: ${if:${task.env.hasCameraSensors},'camera','viewer'} # options are `camera` to use cameras setup in environment, `viewer` to record video from viewer, or `none` to not create cameras in environment (useful if record_video is False since it will disable unneeded environment camera rendering)`
record_entire_train_video: False # set to true to record video for entire training process. set to false to use freq and video len parameters. Entire test video will always be recorded
capture_train_video_freq: 1000 # measured in environment steps; divide by PPO config horizon length to get # epochs this corresponds to
capture_train_video_len: 100

# set default task and default training config based on task
defaults:
  - task: LeapHandRot
  - train: ${task}PPO
  - model: ${task}
  - embodiment: ${task}
  - override hydra/job_logging: disabled
  - _self_

# set the directory where the output files get saved
hydra:
  output_subdir: null
  run:
    dir: .

